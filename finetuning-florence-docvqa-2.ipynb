{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30733,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-07-06T03:45:23.412893Z","iopub.execute_input":"2024-07-06T03:45:23.413529Z","iopub.status.idle":"2024-07-06T03:45:23.421168Z","shell.execute_reply.started":"2024-07-06T03:45:23.413486Z","shell.execute_reply":"2024-07-06T03:45:23.419913Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"**Installing Necessary Dependencies:**","metadata":{}},{"cell_type":"code","source":"!pip install datasets flash_attn timm einops transformers pillow huggingface_hub","metadata":{"execution":{"iopub.status.busy":"2024-07-06T03:48:58.401007Z","iopub.execute_input":"2024-07-06T03:48:58.401305Z","iopub.status.idle":"2024-07-06T03:49:27.576095Z","shell.execute_reply.started":"2024-07-06T03:48:58.401278Z","shell.execute_reply":"2024-07-06T03:49:27.574961Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Requirement already satisfied: datasets in /opt/conda/lib/python3.10/site-packages (2.19.2)\nCollecting flash_attn\n  Downloading flash_attn-2.5.9.post1.tar.gz (2.6 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.6/2.6 MB\u001b[0m \u001b[31m45.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: timm in /opt/conda/lib/python3.10/site-packages (1.0.3)\nCollecting einops\n  Downloading einops-0.8.0-py3-none-any.whl.metadata (12 kB)\nRequirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.41.2)\nRequirement already satisfied: pillow in /opt/conda/lib/python3.10/site-packages (9.5.0)\nRequirement already satisfied: huggingface_hub in /opt/conda/lib/python3.10/site-packages (0.23.2)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from datasets) (3.13.1)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from datasets) (1.26.4)\nRequirement already satisfied: pyarrow>=12.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (14.0.2)\nRequirement already satisfied: pyarrow-hotfix in /opt/conda/lib/python3.10/site-packages (from datasets) (0.6)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.3.8)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets) (2.2.1)\nRequirement already satisfied: requests>=2.32.1 in /opt/conda/lib/python3.10/site-packages (from datasets) (2.32.3)\nRequirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.10/site-packages (from datasets) (4.66.4)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets) (3.4.1)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets) (0.70.16)\nRequirement already satisfied: fsspec<=2024.3.1,>=2023.1.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]<=2024.3.1,>=2023.1.0->datasets) (2024.3.1)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets) (3.9.1)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from datasets) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from datasets) (6.0.1)\nRequirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (from flash_attn) (2.1.2)\nRequirement already satisfied: torchvision in /opt/conda/lib/python3.10/site-packages (from timm) (0.16.2)\nRequirement already satisfied: safetensors in /opt/conda/lib/python3.10/site-packages (from timm) (0.4.3)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2023.12.25)\nRequirement already satisfied: tokenizers<0.20,>=0.19 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.19.1)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface_hub) (4.9.0)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (23.2.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.4)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.9.3)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.4.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.3)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->datasets) (3.1.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.1->datasets) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.1->datasets) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.1->datasets) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.1->datasets) (2024.2.2)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2023.3.post1)\nRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2023.4)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch->flash_attn) (1.12.1)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch->flash_attn) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch->flash_attn) (3.1.2)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch->flash_attn) (2.1.3)\nRequirement already satisfied: mpmath<1.4.0,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch->flash_attn) (1.3.0)\nDownloading einops-0.8.0-py3-none-any.whl (43 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.2/43.2 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hBuilding wheels for collected packages: flash_attn\n  Building wheel for flash_attn (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for flash_attn: filename=flash_attn-2.5.9.post1-cp310-cp310-linux_x86_64.whl size=120576656 sha256=0f3dddbf9bc350ea6b0306ec5ca5fee71b57fe1f06e6b72672690793d9dad2ce\n  Stored in directory: /root/.cache/pip/wheels/cc/ad/f6/7ccf0238790d6346e9fe622923a76ec218e890d356b9a2754a\nSuccessfully built flash_attn\nInstalling collected packages: einops, flash_attn\nSuccessfully installed einops-0.8.0 flash_attn-2.5.9.post1\n","output_type":"stream"}]},{"cell_type":"code","source":"from huggingface_hub import login\nlogin()","metadata":{"execution":{"iopub.status.busy":"2024-07-06T03:49:27.577828Z","iopub.execute_input":"2024-07-06T03:49:27.578107Z","iopub.status.idle":"2024-07-06T03:49:27.860221Z","shell.execute_reply.started":"2024-07-06T03:49:27.578080Z","shell.execute_reply":"2024-07-06T03:49:27.859105Z"},"trusted":true},"execution_count":2,"outputs":[{"output_type":"display_data","data":{"text/plain":"VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ded45b608702402b8f58d983d5b4bb0d"}},"metadata":{}}]},{"cell_type":"code","source":"from datasets import load_dataset\nfrom transformers import AutoModelForCausalLM, AutoProcessor, AdamW, get_scheduler\nimport torch\nimport os\nfrom torch.utils.data import DataLoader, Dataset\nfrom tqdm import tqdm\n\n# 1. Configuration\ndata = load_dataset(\"HuggingFaceM4/DocumentVQA\")\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = AutoModelForCausalLM.from_pretrained(\"microsoft/Florence-2-base-ft\", trust_remote_code=True, revision='refs/pr/6').to(device)\nprocessor = AutoProcessor.from_pretrained(\"microsoft/Florence-2-base-ft\", trust_remote_code=True, revision='refs/pr/6')\ntorch.cuda.empty_cache()\n\n# 2. Before Training the Model\ndef run_example(task_prompt, text_input, image):\n    prompt = task_prompt + text_input\n\n    # Ensure the image is in RGB mode\n    if image.mode != \"RGB\":\n        image = image.convert(\"RGB\")\n\n    inputs = processor(text=prompt, images=image, return_tensors=\"pt\").to(device)\n    generated_ids = model.generate(\n        input_ids=inputs[\"input_ids\"],\n        pixel_values=inputs[\"pixel_values\"],\n        max_new_tokens=1024,\n        num_beams=3\n    )\n    generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n    # Assume there is a method in processor to handle generation post-processing\n    parsed_answer = processor.post_process_generation(generated_text, task=task_prompt, image_size=(image.width, image.height))\n    return parsed_answer\n\nfor idx in range(3):\n    print(run_example(\"DocVQA\", 'What do you see in this image?', data['train'][idx]['image']))\n\n# 3. Dataset Preparation\nclass DocVQADataset(Dataset):\n    def __init__(self, data):\n        self.data = data\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        example = self.data[idx]\n        question = \"<DocVQA>\" + example['question']\n        first_answer = example['answers'][0]\n        image = example['image']\n        if image.mode != \"RGB\":\n            image = image.convert(\"RGB\")\n        return question, first_answer, image\n\ntrain_dataset = DocVQADataset(data['train'].select(range(1000)))\nval_dataset = DocVQADataset(data['validation'].select(range(100)))\n\n# Convert to Embeddings\ndef collate_fn(batch):\n    questions, answers, images = zip(*batch)\n    inputs = processor(text=list(questions), images=list(images), return_tensors=\"pt\", padding=True).to(device)\n    return inputs, answers\n\nbatch_size = 1\nnum_workers = 0\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn, num_workers=num_workers)\nval_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn, num_workers=num_workers)\n\n# 4. Training Model\ndef train_model(train_loader, val_loader, model, processor, epochs=10, lr=1e-6):\n    optimizer = AdamW(model.parameters(), lr=lr)\n    num_training_steps = epochs * len(train_loader)\n    lr_scheduler = get_scheduler(\n        name=\"linear\",\n        optimizer=optimizer,\n        num_warmup_steps=0,\n        num_training_steps=num_training_steps,\n    )\n\n    for epoch in range(epochs):\n        model.train()\n        train_loss = 0\n        for batch in tqdm(train_loader, desc=f\"Training Epoch {epoch + 1}/{epochs}\"):\n            inputs, answers = batch\n\n            input_ids = inputs[\"input_ids\"]\n            pixel_values = inputs[\"pixel_values\"]\n\n            # Ensure labels are correctly processed and moved to the device\n            labels = processor.tokenizer(text=list(answers), return_tensors=\"pt\", padding=True, return_token_type_ids=False).input_ids.to(device)\n            \n            outputs = model(input_ids=input_ids, pixel_values=pixel_values, labels=labels)\n            loss = outputs.loss\n\n            loss.backward()\n            optimizer.step()\n            lr_scheduler.step()\n            optimizer.zero_grad()\n\n            train_loss += loss.item()\n\n        avg_train_loss = train_loss / len(train_loader)\n        print(f\"Average Training Loss: {avg_train_loss}\")\n\n        # Validation phase\n        model.eval()\n        val_loss = 0\n        with torch.no_grad():\n            for batch in tqdm(val_loader, desc=f\"Validation Epoch {epoch + 1}/{epochs}\"):\n                inputs, answers = batch\n\n                input_ids = inputs[\"input_ids\"]\n                pixel_values = inputs[\"pixel_values\"]\n\n                # Ensure labels are correctly processed and moved to the device\n                labels = processor.tokenizer(text=list(answers), return_tensors=\"pt\", padding=True, return_token_type_ids=False).input_ids.to(device)\n\n                outputs = model(input_ids=input_ids, pixel_values=pixel_values, labels=labels)\n                loss = outputs.loss\n\n                val_loss += loss.item()\n\n        avg_val_loss = val_loss / len(val_loader)\n        print(f\"Average Validation Loss: {avg_val_loss}\")\n\n        # Save model checkpoint\n        output_dir = f\"./model_checkpoints/epoch_{epoch+1}\"\n        os.makedirs(output_dir, exist_ok=True)\n        model.save_pretrained(output_dir)\n        processor.save_pretrained(output_dir)\n\n# Freeze vision tower parameters\nfor param in model.vision_tower.parameters():\n    param.requires_grad = False\n\n# Train the model\ntrain_model(train_loader, val_loader, model, processor, epochs=1)","metadata":{"execution":{"iopub.status.busy":"2024-07-06T03:51:37.193077Z","iopub.execute_input":"2024-07-06T03:51:37.193797Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stderr","text":"2024-07-06 03:51:42.352933: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-07-06 03:51:42.353063: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-07-06 03:51:42.466690: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading readme:   0%|          | 0.00/806 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"710941f3d8194bce81a9ec2ac3127893"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Resolving data files:   0%|          | 0/38 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6b4acfb2a7c7454fbe6caf45b89ec0d7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Resolving data files:   0%|          | 0/17 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"57921341741e4c1ca721be63a893ae03"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Resolving data files:   0%|          | 0/17 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e7d5345d23244d83939bbb48073194ba"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Resolving data files:   0%|          | 0/38 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"03fec4baf8504db7bd51408253be468e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Resolving data files:   0%|          | 0/17 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c7f247feb2cd44aebe58368b66dd257e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Resolving data files:   0%|          | 0/17 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"be375460805b4266bc30d8c9386491c9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0/38 [00:00<?, ?files/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bcccaffc41b0435aa5de6aac4a528187"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0/17 [00:00<?, ?files/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"df5a62775a7942d5ab54bddf0613575e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0/17 [00:00<?, ?files/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7a76392a953143c2bb337fcb77331729"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/39463 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"92309353faa1474aa907a0c473aab925"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split:   0%|          | 0/5349 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ac69d7b8e2484eaa81a20e9772ff1511"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/5188 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7851745dc02c4f61aa59c13b6159072d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading dataset shards:   0%|          | 0/51 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"82d8a97f480145c3bf5847259e47f3d3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/2.43k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"44d1d8dffb0e49f4aef9d22c3cf7aa7b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"configuration_florence2.py:   0%|          | 0.00/15.1k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"373eb555448d437492a8a02c5ccca3a9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"modeling_florence2.py:   0%|          | 0.00/127k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"18ee6c684ef44b66a562f1f7dbf7f483"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/464M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8f0772ce8d534ba1a4a032ef3ee947b6"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n  return self.fget.__get__(instance, owner)()\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"preprocessor_config.json:   0%|          | 0.00/806 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a52494b4d17c4d63a445f70e806c3652"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"processing_florence2.py:   0%|          | 0.00/46.4k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"99d0b6adc6e544c7a0187736400b307d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/34.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"67dde943220246fe93e321ce2886940b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/1.10M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"594c0945116c47cf8955f1dec421217b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f4833a2ba12c4e6f8ce10a2a7584ba82"}},"metadata":{}},{"name":"stderr","text":"A new version of the following files was downloaded from https://huggingface.co/microsoft/Florence-2-base-ft:\n- configuration_florence2.py\n. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n","output_type":"stream"},{"name":"stdout","text":"{'DocVQA': 'unanswerable'}\n{'DocVQA': 'unanswerable'}\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:588: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"{'DocVQA': '499150498'}\n","output_type":"stream"},{"name":"stderr","text":"Training Epoch 1/1:   3%|▎         | 29/1000 [00:11<06:06,  2.65it/s]","output_type":"stream"}]},{"cell_type":"code","source":"# 5. Save to HuggingFace Hub\nmodel.push_to_hub(\"mihirinamdar/Florence-2-FT-DocVQA\")\nprocessor.push_to_hub(\"mihirinamdar/Florence-2-FT-DocVQA\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}